{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cd /data/espnet/docker/prebuilt/devel/gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据\n",
    "data_path = 'm_0.65_0.85'\n",
    "fileNames = glob(data_path+'/*0.txt')\n",
    "_idx = np.arange(len(fileNames))\n",
    "np.random.shuffle(fileNames)\n",
    "fileNames[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def dump(sr=None):\n",
    "\n",
    "    if sr is None: # 从配置文件中获取 sr\n",
    "        with open(data_path+'/输入参数文本.txt') as _f:\n",
    "            _paras = _f.readlines()\n",
    "            sr = int(_paras[2].split('\\t')[0]) + 1\n",
    "            print('sr=',sr)\n",
    "\n",
    "    # --------------------------------------------------------------通用begin\n",
    "    with open(fileNames[0]) as _f:\n",
    "        simple_size = sys.getsizeof(_f.read())/1024/1024 # Mb\n",
    "\n",
    "\n",
    "    filename = fileNames\n",
    "# --------------------------------------------------------------通用end\n",
    "    i1 = np.zeros((len(filename), sr))\n",
    "    i2 = np.zeros((len(filename), sr))\n",
    "    dop = np.zeros((len(filename), sr))\n",
    "    M,ALPHA = [], []\n",
    "    t0 = time.time()\n",
    "    L = len(filename)\n",
    "    for idx, i in enumerate(filename):\n",
    "        baseName = os.path.basename(i) \n",
    "        m,alpha = baseName.split(',')\n",
    "        alpha = '.'.join(alpha.split('.')[:-1])\n",
    "        m = m.split('=')[-1]\n",
    "        alpha=alpha.split('=')[-1]\n",
    "        M.append(m)\n",
    "        ALPHA.append(alpha)\n",
    "\n",
    "        i1[idx] = pd.read_table(i, encoding='gb2312',sep='\\t',index_col=None).I1.to_numpy().astype(np.float32)\n",
    "        i2[idx] = pd.read_table(i, encoding='gb2312',sep='\\t',index_col=None).I2.to_numpy().astype(np.float32)\n",
    "        dop[idx] = pd.read_table(i, encoding='gb2312',sep='\\t',index_col=None).DOP.to_numpy().astype(np.float32)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        per_current = (idx+1)/L\n",
    "        cost_time = t1-t0\n",
    "        Totle_time = cost_time/per_current\n",
    "#         line_ = 100*(per_current)//2 * '='\n",
    "        print(f'\\r{cost_time:.1f}s |{Totle_time-cost_time:.1f}s ',end='', flush=True)\n",
    "    \n",
    "    M, ALPHA = np.array(M).astype('float32'), np.array(ALPHA).astype('float32')\n",
    "    # --------------------------------------------------------------- 通用begin\n",
    "    out_path = '0.65_out_shuffle'\n",
    "    np.save(os.path.join(out_path, f'i1_{0}.npy'),i1)\n",
    "    np.save(os.path.join(out_path, f'i2_{0}.npy'),i2)\n",
    "    np.save(os.path.join(out_path, f'dop_{0}.npy'),dop)\n",
    "    np.savez(os.path.join(out_path, f'M_ALPHA_{0}.npz'),M=M,ALPHA=ALPHA)\n",
    "\n",
    "    # --------------------------------------------------------------- 通用end\n",
    "    return out_path, sr\n",
    "\n",
    "target_path, sr = dump(sr=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path, sr = '0.65_out_shuffle', 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.65_out_shuffle/i1_0.npy'] \n",
      " ['0.65_out_shuffle/i2_0.npy'] \n",
      " ['0.65_out_shuffle/M_ALPHA_0.npz']\n",
      "(50500, 2048) (50500, 2048) (50500,)\n"
     ]
    }
   ],
   "source": [
    "# # load\n",
    "# def load_data(path):\n",
    "#     fileNames = glob(os.path.join(path,'*'))\n",
    "    \n",
    "#     name_i1 = [i for i in fileNames if 'i1' in i]\n",
    "#     name_i2 = [i for i in fileNames if 'i2' in i]\n",
    "#     name_dop = [i for i in fileNames if 'dop' in i]\n",
    "#     name_m_alpha = [i for i in fileNames if 'M_ALPHA' in i]\n",
    "#     print(name_i1,'\\n',name_i2,'\\n',name_m_alpha)\n",
    "#     L = len(name_dop) # 3种文件数目相同\n",
    "#     c = 0\n",
    "#     while True:\n",
    "#         i1 = np.load(name_i1[c%L])\n",
    "#         i2 = np.load(name_i2[c%L])\n",
    "#         dop = np.load(name_dop[c%L])\n",
    "#         res = np.load(name_m_alpha[c%L])\n",
    "#         M, ALPHA = res['M'], res['ALPHA']\n",
    "#         print(i1.shape, i2.shape, M.shape)\n",
    "#         c += 1\n",
    "#         yield i1, i2, dop, M, ALPHA\n",
    "        \n",
    "\n",
    "# data = load_data(target_path)\n",
    "# i1, i2, dop, M, ALPHA = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50500, 2048) (50500, 2048) (50500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50500, 2048), (50500, 2048), (50500, 2048), (50500,), (50500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i1, i2, dop, M, ALPHA = next(data)\n",
    "# i1.shape, i2.shape, dop.shape, M.shape, ALPHA.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.822, 50.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# IDX = 1000\n",
    "# plt.figure(figsize=(16,8))\n",
    "# plt.plot(np.log(i1[IDX]))\n",
    "# M[IDX], ALPHA[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generateData(batch,theta1, theta2,i1, i2, dop,M, ALPHA, sr, L=i1.shape[0]):\n",
    "#     S = np.arange(L-1)\n",
    "\n",
    "#     d_the = (sr-1)/180\n",
    "#     th1, th2 = int(theta1*d_the), int(theta2*d_the)\n",
    "#     SR = th2-th1\n",
    "#     while True:\n",
    "#         np.random.shuffle(S)\n",
    "#         series = S[:batch]\n",
    "#         yield SR,\\\n",
    "#         i1[series, th1:th2].astype(np.float32)[...,None], \\\n",
    "#         i2[series, th1:th2].astype(np.float32)[...,None], \\\n",
    "#         dop[series, th1:th2].astype(np.float32)[...,None], \\\n",
    "#         np.array(M)[series].astype(np.float32)[...,None], \\\n",
    "#         np.array(ALPHA)[series].astype(np.float32)[...,None]\n",
    "        \n",
    "# theta1, theta2 = 15, 36\n",
    "\n",
    "# data = generateData(15, theta1,theta2,i1,i2,dop,M,ALPHA,sr)\n",
    "# SR = next(data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n",
      "(15, 239, 1)\n",
      "(15, 239, 1)\n",
      "(15, 239, 1)\n",
      "(15, 1)\n",
      "(15, 1)\n"
     ]
    }
   ],
   "source": [
    "print(next(data)[0])\n",
    "print(next(data)[1].shape) # i1\n",
    "print(next(data)[2].shape) # i2\n",
    "print(next(data)[3].shape) # dop\n",
    "print(next(data)[4].shape) # M\n",
    "print(next(data)[5].shape) # ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# tf.test.is_gpu_available()\n",
    "from tensorflow.keras import Input,Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Conv2D,Conv1D,Convolution1D,\\\n",
    "Flatten, BatchNormalization,Input,Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU, LayerNormalization\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, MAE, KLD, MSLE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model 1\n",
    "# def model1(SR):\n",
    "#     input_x = Input(shape=(SR, 1))\n",
    "#     # x = LayerNormalization()(input_x)\n",
    "#     x = input_x\n",
    "\n",
    "#     x = Conv1D(3, 4, strides=2,padding='same')(x)\n",
    "#     # x = BatchNormalization()(x)\n",
    "#     x = ReLU()(x)\n",
    "#     x = Conv1D(3, 3, strides=2,padding='same')(x)\n",
    "\n",
    "#     x = ReLU()(x)\n",
    "#     x = Conv1D(3, 3, strides=2,padding='same')(x)\n",
    "\n",
    "#     x = ReLU()(x)\n",
    "#     x = Conv1D(3, 3, strides=2,padding='same')(x)\n",
    "\n",
    "#     x = ReLU()(x)\n",
    "#     x = Conv1D(3, 3, strides=2,padding='same')(x)\n",
    "\n",
    "#     x = ReLU()(x)\n",
    "#     x = Flatten()(x)\n",
    "\n",
    "#     x = Dense(100,'relu')(x)\n",
    "#     rm = Dense(1,'sigmoid')(x)\n",
    "#     alpha = Dense(1,'relu')(x)\n",
    "#     dop = Dense(1, 'tanh')(x)\n",
    "#     model_m = Model(input_x, rm)\n",
    "#     model_alpha = Model(input_x, alpha)\n",
    "#     model_dop = Model(input_x, dop)\n",
    "#     return model_m, model_alpha, model_dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model 2\n",
    "# def model2(SR):\n",
    "#     input_x = Input(shape=(SR, 1))\n",
    "#     # x = LayerNormalization()(input_x)\n",
    "    \n",
    "#     # convolution part\n",
    "#     cx = input_x\n",
    "#     cx = Conv1D(1, SR//10, strides=2,padding='valid', activation='relu')(cx)\n",
    "#     cx = Conv1D(1, 3, strides=2,padding='valid', activation='relu')(cx)\n",
    "#     cx = Conv1D(1, 3, strides=2,padding='valid', activation='relu')(cx)\n",
    "\n",
    "#     cx = Flatten()(cx)\n",
    "    \n",
    "#     # full connection part\n",
    "#     fx = input_x\n",
    "#     fx = Flatten()(fx)\n",
    "#     fx = Dense(SR//20,'relu')(fx)\n",
    "\n",
    "# #     x = fx\n",
    "\n",
    "#     x = tf.concat([fx, cx], axis=1)\n",
    "#     print(x.shape)\n",
    "#     rm = Dense(1,'sigmoid')(x)\n",
    "#     alpha = Dense(1, 'relu')(x)\n",
    "#     dop = Dense(1, 'tanh')(x)\n",
    "    \n",
    "#     model_m = Model(input_x, rm)\n",
    "#     model_alpha = Model(input_x, alpha)\n",
    "#     model_dop = Model(input_x, dop)\n",
    "#     return model_m, model_alpha, model_dop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model 3\n",
    "# def model3(SR):\n",
    "#     input_x = Input(shape=(SR, 1))\n",
    "#     # x = LayerNormalization()(input_x)\n",
    "    \n",
    "#     # convolution part\n",
    "#     x = Flatten()(input_x)\n",
    "    \n",
    "#     x = Dense(SR*2,'relu')(x)\n",
    "#     x = Dense(SR*2,'relu')(x)\n",
    "#     x = Dense(SR*2,'relu')(x)\n",
    "#     x = Dense(SR*2,'relu')(x)\n",
    "#     x = Dense(SR,'relu')(x)\n",
    "#     x = Dense(SR,'relu')(x)\n",
    "\n",
    "#     rm = Dense(1,'sigmoid')(x)\n",
    "#     alpha = Dense(1, 'relu')(x)\n",
    "#     dop = Dense(1, 'sigmoid')(x)\n",
    "    \n",
    "#     model_m = Model(input_x, rm)\n",
    "#     model_alpha = Model(input_x, alpha)\n",
    "#     model_dop = Model(input_x, dop)\n",
    "#     return model_m, model_alpha, model_dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 239, 1)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 239)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 478)               114720    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 478)               228962    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 478)               228962    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 478)               228962    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 239)               114481    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 239)               57360     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 240       \n",
      "=================================================================\n",
      "Total params: 973,687\n",
      "Trainable params: 973,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_rm, model_alpha, model_dop = model3(SR)\n",
    "\n",
    "# model = model_alpha\n",
    "# # model = model_rm\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss= MSE,\n",
    "#               optimizer=Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "# #               optimizer=SGD()\n",
    "#              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.65_out_shuffle/i1_0.npy'] \n",
      " ['0.65_out_shuffle/i2_0.npy'] \n",
      " ['0.65_out_shuffle/M_ALPHA_0.npz']\n",
      "(50500, 2048) (50500, 2048) (50500,)\n",
      "loss: 0.00018266504048369825\n"
     ]
    }
   ],
   "source": [
    "# # train with rm \n",
    "# batch = 900\n",
    "# theta1, theta2 = 15, 36\n",
    "# DATA = load_data(target_path)\n",
    "# Big_loop = 0\n",
    "# while Big_loop < 1:\n",
    "\n",
    "#     i1,i2,dop,M,ALPHA = next(DATA)\n",
    "#     Big_loop += 1\n",
    "#     data = generateData(batch,theta1,theta2,i1,i2,dop,M,ALPHA,sr, i1.shape[0])\n",
    "    \n",
    "#     # 使用集成度较低的 Model.train_on_batch方法，自行设定batch数据　和　epochs\n",
    "#     for step in range(5000):\n",
    "#         train_data = next(data)\n",
    "#     #     cost = model.train_on_batch(train_data[1], train_data[-2]) # i1\n",
    "#     #     cost = model.train_on_batch(train_data[2], train_data[-2]) # i2\n",
    "#         cost = model.train_on_batch(train_data[3], train_data[-2]) # dop\n",
    "\n",
    "#         if step%500==0:\n",
    "#         print(f'loss: {cost}')\n",
    "#     model.save(f'rm_model_{step}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 239, 1), (10, 1))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch=10\n",
    "# data = generateData(batch,theta1,theta2,i1,i2,dop,M,ALPHA,sr, i1.shape[0])\n",
    "# train_data = next(data)\n",
    "# test_dop, test_rm = train_data[3], train_data[-2]\n",
    "# test_dop.shape, test_rm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7758099] [0.774]\n",
      "[0.81077933] [0.808]\n",
      "[0.7611233] [0.76]\n",
      "[0.7962868] [0.796]\n",
      "[0.8339187] [0.832]\n",
      "[0.75567824] [0.654]\n",
      "[0.81595075] [0.816]\n",
      "[0.68885803] [0.688]\n",
      "[0.78161675] [0.784]\n",
      "[0.6986482] [0.698]\n"
     ]
    }
   ],
   "source": [
    "# for i, j in zip(model.predict(test_dop), test_rm):\n",
    "#     print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple(theta1, theta2, alpha, rm, noise=False):\n",
    "    test_i1, test_i2, test_dop = gen(alpha = alpha, rm = rm, N_ta = sr-1) # \n",
    "    \n",
    "    d_the = (sr-1)/180\n",
    "    th1, th2 = int(theta1*d_the), int(theta2*d_the)\n",
    "    SR = th2-th1\n",
    "    test_i1, test_i2, test_dop = test_i1[th1:th2], test_i2[th1:th2], test_dop[th1:th2]\n",
    "    \n",
    "    if noise:\n",
    "        noise = np.random.rand(SR, 1) * 0.05\n",
    "        noise_dop = (np.random.rand(SR, 1)-0.5) * 0.05\n",
    "        test_i1, test_i2, = test_i1+noise, test_i2+noise\n",
    "        test_dop = test_dop + noise_dop\n",
    "    return test_i1, test_i2, test_dop \n",
    "\n",
    "def test_batch():\n",
    "    RM = [0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772]\n",
    "    ALPHA=[5.2,  20.2, 50.2, 70.2, 90.2, 100.2,140.2]\n",
    "    RES = np.zeros((len(RM),len(ALPHA)))\n",
    "    for ir, rm in enumerate(RM):\n",
    "        for ia, alpha in enumerate(ALPHA):\n",
    "            test_dop = test_simple(theta1, theta2,alpha,rm, noise=0)[2]\n",
    "            pred_rm = model.predict(test_dop[None,])\n",
    "            \n",
    "            RES[ir][ia] = pred_rm\n",
    "    \n",
    "    mean_rm = np.mean(RES, axis=1)\n",
    "    std_rm = np.std(RES, axis=1)\n",
    "    return std_rm, mean_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generateData(alpha,rm,im =0.0,N_ta = 2**13-1):\n",
    "    origin_path = os.getcwd()\n",
    "    os.chdir(r\"C:\\Users\\beidongjiedeguang\\Desktop\\测试文件夹\")\n",
    "#     os.chdir(r\"C:\\Users\\Administrator\\OneDrive\\Mie测试文件夹\")\n",
    "    exeName = \"Mie_Calculation.exe\"\n",
    "    txtName = '输入参数文本.txt'\n",
    "\n",
    "    Ta_min, Ta_max = 0,180\n",
    "    with open(txtName,'w') as f1:\n",
    "        f1.write('{}\\n{:.3f}\\t{:.3f}\\n{}\\t{:.2f}\\t{:.2f}'\n",
    "                .format(alpha,rm,im,N_ta,Ta_min,Ta_max))\n",
    "\n",
    "    os.system(exeName)\n",
    "    dataName = \"散射光强角分布.txt\"\n",
    "    # 没想到c写出来的不是utf-8编码的，所以这里encoding='gbk'，后面这个delimiter随便要不要\n",
    "    data = pd.read_csv(dataName,sep ='\\t',engine='python',encoding='gbk',delimiter='\\s+')\n",
    "    os.remove(dataName)\n",
    "    os.chdir(origin_path)\n",
    "    return data\n",
    "\n",
    "def gen(alpha = 900, rm = 1.7, im= 0.0, N_ta = 2**13-1):\n",
    "    data = test_generateData(alpha,rm,im, N_ta = N_ta)\n",
    "    theta = data['散射角'].to_numpy()\n",
    "    i1, i2, DOP = data['I1'].to_numpy(), data['I2'].to_numpy(), data['可视度'].to_numpy()\n",
    "    return i1[...,None], i2[...,None], DOP[...,None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('rm_model_5000.h5') \n",
    "theta1, theta2 = 15, 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([39.67944779, 39.67944779, 39.67944779, 39.67944779, 39.67944779,\n",
       "        39.67944779, 39.67944779]),\n",
       " array([65.92524263, 65.92524263, 65.92524263, 65.92524263, 65.92524263,\n",
       "        65.92524263, 65.92524263]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1, theta2 = 15, 36\n",
    "test_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70198685]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_alpha, test_rm = 39, 0.7\n",
    "test_i1, test_i2, test_dop = test_simple(theta1, theta2, alpha=test_alpha, rm=test_rm, noise=False)\n",
    "model.predict(test_dop[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('alpha_model_5000.h5') \n",
    "theta1, theta2 = 15, 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[113.35775]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_alpha, test_rm = 110, 0.8\n",
    "test_i1, test_i2, test_dop = test_simple(theta1, theta2, alpha=test_alpha, rm=test_rm, noise=False)\n",
    "model.predict(test_dop[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
