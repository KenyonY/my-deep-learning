{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "在介绍PyTorch之前，我们将首先使用numpy实现网络。\n",
    "\n",
    "Numpy提供了一个n维数组对象，以及许多用于操作这些数组的函数。Numpy是科学计算的通用框架; 它对计算图，深度学习或渐变都一无所知。但是，通过使用numpy操作手动实现网络的前向和后向传递，我们可以轻松地使用numpy将两层网络适配到随机数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24208137.623141058\n",
      "1 21850789.227255687\n",
      "2 20132483.659388423\n",
      "3 18809718.253678717\n",
      "4 17739305.687064625\n",
      "5 16835282.08636658\n",
      "6 16044345.75576635\n",
      "7 15333861.612287212\n",
      "8 14684277.700018479\n",
      "9 14082873.473491767\n",
      "10 13520819.090716593\n",
      "11 12992805.307806227\n",
      "12 12494724.894214015\n",
      "13 12023931.93026122\n",
      "14 11577774.984947763\n",
      "15 11153811.397413373\n",
      "16 10750404.057258168\n",
      "17 10366267.669776822\n",
      "18 10000518.153287828\n",
      "19 9651780.368189756\n",
      "20 9319203.428830683\n",
      "21 9001938.173719611\n",
      "22 8698757.798353303\n",
      "23 8408929.687612012\n",
      "24 8131594.525932021\n",
      "25 7866145.212545418\n",
      "26 7611917.525715859\n",
      "27 7368258.793255724\n",
      "28 7134619.624340925\n",
      "29 6910538.356527424\n",
      "30 6695460.0783165125\n",
      "31 6488975.710196834\n",
      "32 6290684.549237605\n",
      "33 6100250.016120296\n",
      "34 5917180.034794411\n",
      "35 5741172.722602271\n",
      "36 5571907.123673895\n",
      "37 5409181.181129552\n",
      "38 5252615.916038133\n",
      "39 5101879.317655962\n",
      "40 4956848.341027949\n",
      "41 4817117.203803131\n",
      "42 4682424.337819265\n",
      "43 4552552.354583254\n",
      "44 4427290.216485217\n",
      "45 4306608.920590263\n",
      "46 4190167.2602110244\n",
      "47 4077726.6646525064\n",
      "48 3969155.5874199504\n",
      "49 3864318.5888762283\n",
      "50 3763107.996944284\n",
      "51 3665329.267990352\n",
      "52 3570794.1504732277\n",
      "53 3479412.9376953584\n",
      "54 3391004.137379514\n",
      "55 3305341.7699486073\n",
      "56 3222442.9343338115\n",
      "57 3142267.424642534\n",
      "58 3064701.079096294\n",
      "59 2989581.929864878\n",
      "60 2916924.0274807056\n",
      "61 2846523.485046413\n",
      "62 2778292.494791896\n",
      "63 2712161.3249105467\n",
      "64 2648033.7630760055\n",
      "65 2585846.1799336504\n",
      "66 2525525.9106098823\n",
      "67 2466996.1566813583\n",
      "68 2410201.3659106665\n",
      "69 2355119.273481394\n",
      "70 2301739.127913981\n",
      "71 2249932.169916177\n",
      "72 2199608.4102098793\n",
      "73 2150734.4556431887\n",
      "74 2103258.8828432127\n",
      "75 2057155.9848223033\n",
      "76 2012374.7182213315\n",
      "77 1968841.0707686525\n",
      "78 1926511.8228789982\n",
      "79 1885353.8971373225\n",
      "80 1845319.810721079\n",
      "81 1806358.3598140376\n",
      "82 1768447.469775103\n",
      "83 1731549.2686223055\n",
      "84 1695685.0648718267\n",
      "85 1660781.4029903954\n",
      "86 1626788.3649775211\n",
      "87 1593688.2659990883\n",
      "88 1561452.7842388533\n",
      "89 1530051.6640817288\n",
      "90 1499454.6846219394\n",
      "91 1469632.9571771664\n",
      "92 1440573.5160153324\n",
      "93 1412246.5201132437\n",
      "94 1384640.3379260604\n",
      "95 1357739.0650227459\n",
      "96 1331515.1079916526\n",
      "97 1305933.3623256788\n",
      "98 1280972.7188621862\n",
      "99 1256622.0901134736\n",
      "100 1232856.1171604842\n",
      "101 1209658.0323114134\n",
      "102 1187016.5630091242\n",
      "103 1164914.0918858303\n",
      "104 1143340.2081622232\n",
      "105 1122274.6586731356\n",
      "106 1101704.6296253835\n",
      "107 1081607.5650440105\n",
      "108 1061976.182473347\n",
      "109 1042794.949995426\n",
      "110 1024051.1296403487\n",
      "111 1005734.6705578123\n",
      "112 987838.237350695\n",
      "113 970343.8179220798\n",
      "114 953241.3813950593\n",
      "115 936524.6416938871\n",
      "116 920174.5615494342\n",
      "117 904186.8577459757\n",
      "118 888551.3806660281\n",
      "119 873278.2146069179\n",
      "120 858341.5875221628\n",
      "121 843727.6053273604\n",
      "122 829426.4493753628\n",
      "123 815431.3914397471\n",
      "124 801744.5864354561\n",
      "125 788351.9576119886\n",
      "126 775240.5729412421\n",
      "127 762405.7427029696\n",
      "128 749847.1170797008\n",
      "129 737547.5754562642\n",
      "130 725497.3525805218\n",
      "131 713695.2555233664\n",
      "132 702139.6177527301\n",
      "133 690818.6906232049\n",
      "134 679727.5986544675\n",
      "135 668855.9137817469\n",
      "136 658202.1278254085\n",
      "137 647764.3584200512\n",
      "138 637538.6516493747\n",
      "139 627517.4279709297\n",
      "140 617698.0893700578\n",
      "141 608069.0875492268\n",
      "142 598630.4324274044\n",
      "143 589375.4504207205\n",
      "144 580296.5652665314\n",
      "145 571390.986826761\n",
      "146 562658.419615152\n",
      "147 554092.2104251275\n",
      "148 545688.804408478\n",
      "149 537440.8476713003\n",
      "150 529351.9523471745\n",
      "151 521416.98135344434\n",
      "152 513631.5434901611\n",
      "153 505990.5471663704\n",
      "154 498497.1627745423\n",
      "155 491147.5923721169\n",
      "156 483931.6406758494\n",
      "157 476843.38947225496\n",
      "158 469883.6412088361\n",
      "159 463049.872895327\n",
      "160 456337.46123008174\n",
      "161 449744.8335925073\n",
      "162 443272.4765689342\n",
      "163 436914.2555122312\n",
      "164 430665.0650138262\n",
      "165 424526.22446027014\n",
      "166 418495.2667561687\n",
      "167 412568.2370517374\n",
      "168 406745.7597580683\n",
      "169 401026.2695459289\n",
      "170 395404.4608304824\n",
      "171 389877.49834234524\n",
      "172 384445.7206884302\n",
      "173 379106.1116974405\n",
      "174 373857.15353377396\n",
      "175 368699.16984992137\n",
      "176 363626.1323122926\n",
      "177 358638.88389427343\n",
      "178 353734.3282774904\n",
      "179 348910.13452465995\n",
      "180 344165.9689498586\n",
      "181 339501.5055338499\n",
      "182 334914.9575881803\n",
      "183 330404.6303203763\n",
      "184 325970.28713548265\n",
      "185 321607.46801494376\n",
      "186 317314.3638805726\n",
      "187 313089.9645863186\n",
      "188 308945.4388292845\n",
      "189 304867.2367374883\n",
      "190 300855.53295164765\n",
      "191 296908.3255140006\n",
      "192 293023.03617533384\n",
      "193 289198.75187223137\n",
      "194 285437.3088289652\n",
      "195 281735.8745954329\n",
      "196 278095.26744588895\n",
      "197 274514.0734010876\n",
      "198 270986.82584738976\n",
      "199 267513.6798061986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "# H是隐藏层维度不是层个数，也就是那一层神经元个数\n",
    "N, D_in, H, D_out = 64, 1000, 100,10\n",
    "# D_in和D_out是每个数据对应的维度,所以一批数据的维度就是 （N，D）\n",
    "\n",
    "# 创建随机输入与输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in,H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-7\n",
    "for t in range(200):\n",
    "    # 前向传播：计算预测值y,\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h,0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # 计算损失函数\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t,loss)\n",
    "    \n",
    "    # 反向传播，由链式法则计算w1 w2 对应loss的梯度\n",
    "    grad_y_pred = 2*(y_pred - y)  # loss = sum[(y_pred - y)^2]\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) # y_pred = h_relu.dot(w2)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0 # relu函数小于0部分梯度为0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch: tensors  \n",
    "Numpy是一个很棒的框架，但它不能利用GPU来加速其数值计算。对于现代深度神经网络，GPU通常提供50倍或更高的加速，因此不幸的是，numpy对于现代深度学习来说还不够。\n",
    "\n",
    "在这里，我们介绍最基本的PyTorch概念：Tensor。PyTorch Tensor在概念上与numpy数组相同：Tensor是一个n维数组，PyTorch提供了许多用于在这些Tensors上运算的函数。在幕后，Tensors可以跟踪计算图和渐变，但它们也可用作科学计算的通用工具。\n",
    "\n",
    "与numpy不同，PyTorch Tensors可以利用GPU加速其数值计算。要在GPU上运行PyTorch Tensor，只需将其转换为新的数据类型即可。\n",
    "\n",
    "在这里，我们使用PyTorch Tensors将双层网络与随机数据相匹配。就像上面的numpy示例一样，我们需要手动实现网络中的前向和后向传递："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28769096.0\n",
      "1 26723762.0\n",
      "2 29707274.0\n",
      "3 32923014.0\n",
      "4 31857936.0\n",
      "5 24735096.0\n",
      "6 15197170.0\n",
      "7 7852862.0\n",
      "8 3865950.0\n",
      "9 2053918.25\n",
      "10 1259704.75\n",
      "11 888136.25\n",
      "12 689210.375\n",
      "13 565040.25\n",
      "14 477093.375\n",
      "15 409459.9375\n",
      "16 354895.3125\n",
      "17 309766.75\n",
      "18 271899.375\n",
      "19 239773.03125\n",
      "20 212273.703125\n",
      "21 188583.84375\n",
      "22 168083.84375\n",
      "23 150292.96875\n",
      "24 134726.46875\n",
      "25 121079.5078125\n",
      "26 109071.765625\n",
      "27 98466.65625\n",
      "28 89085.84375\n",
      "29 80753.0234375\n",
      "30 73328.59375\n",
      "31 66704.0\n",
      "32 60783.375\n",
      "33 55472.79296875\n",
      "34 50699.71875\n",
      "35 46411.6875\n",
      "36 42543.03125\n",
      "37 39042.91796875\n",
      "38 35870.75390625\n",
      "39 32992.2890625\n",
      "40 30374.98828125\n",
      "41 27994.421875\n",
      "42 25823.0546875\n",
      "43 23840.9609375\n",
      "44 22029.70703125\n",
      "45 20372.41796875\n",
      "46 18853.55859375\n",
      "47 17460.16015625\n",
      "48 16180.5947265625\n",
      "49 15004.806640625\n",
      "50 13923.0615234375\n",
      "51 12926.84765625\n",
      "52 12009.02734375\n",
      "53 11162.423828125\n",
      "54 10381.048828125\n",
      "55 9659.2939453125\n",
      "56 8991.951171875\n",
      "57 8374.7509765625\n",
      "58 7803.63525390625\n",
      "59 7274.30322265625\n",
      "60 6783.6318359375\n",
      "61 6328.546875\n",
      "62 5906.4638671875\n",
      "63 5514.53515625\n",
      "64 5150.81591796875\n",
      "65 4812.6123046875\n",
      "66 4498.1630859375\n",
      "67 4205.3798828125\n",
      "68 3932.93896484375\n",
      "69 3679.20947265625\n",
      "70 3442.7333984375\n",
      "71 3222.330810546875\n",
      "72 3016.748046875\n",
      "73 2825.024658203125\n",
      "74 2646.13671875\n",
      "75 2479.16357421875\n",
      "76 2323.236083984375\n",
      "77 2177.609130859375\n",
      "78 2041.4969482421875\n",
      "79 1914.3006591796875\n",
      "80 1795.3636474609375\n",
      "81 1684.1923828125\n",
      "82 1580.16015625\n",
      "83 1482.7860107421875\n",
      "84 1391.66552734375\n",
      "85 1306.350830078125\n",
      "86 1226.47412109375\n",
      "87 1151.715087890625\n",
      "88 1081.640380859375\n",
      "89 1015.9788818359375\n",
      "90 954.4366455078125\n",
      "91 896.762451171875\n",
      "92 842.695556640625\n",
      "93 792.0177001953125\n",
      "94 744.4561767578125\n",
      "95 699.84423828125\n",
      "96 658.0013427734375\n",
      "97 618.7236938476562\n",
      "98 581.869140625\n",
      "99 547.2811279296875\n",
      "100 514.800048828125\n",
      "101 484.3106994628906\n",
      "102 455.677001953125\n",
      "103 428.7823486328125\n",
      "104 403.52972412109375\n",
      "105 379.8128967285156\n",
      "106 357.5196533203125\n",
      "107 336.5741882324219\n",
      "108 316.89349365234375\n",
      "109 298.4162292480469\n",
      "110 281.0474548339844\n",
      "111 264.7252197265625\n",
      "112 249.3659210205078\n",
      "113 234.921630859375\n",
      "114 221.33172607421875\n",
      "115 208.55142211914062\n",
      "116 196.52345275878906\n",
      "117 185.2086181640625\n",
      "118 174.5593719482422\n",
      "119 164.53768920898438\n",
      "120 155.0987548828125\n",
      "121 146.22293090820312\n",
      "122 137.86314392089844\n",
      "123 129.99270629882812\n",
      "124 122.58394622802734\n",
      "125 115.60295867919922\n",
      "126 109.02523803710938\n",
      "127 102.83561706542969\n",
      "128 97.00044250488281\n",
      "129 91.50830841064453\n",
      "130 86.32952880859375\n",
      "131 81.44973754882812\n",
      "132 76.85159301757812\n",
      "133 72.51844024658203\n",
      "134 68.43406677246094\n",
      "135 64.5867919921875\n",
      "136 60.958091735839844\n",
      "137 57.53775405883789\n",
      "138 54.312835693359375\n",
      "139 51.2702522277832\n",
      "140 48.402671813964844\n",
      "141 45.69910430908203\n",
      "142 43.14961242675781\n",
      "143 40.74378967285156\n",
      "144 38.47486114501953\n",
      "145 36.33454132080078\n",
      "146 34.31565856933594\n",
      "147 32.41126251220703\n",
      "148 30.614906311035156\n",
      "149 28.919506072998047\n",
      "150 27.319175720214844\n",
      "151 25.809022903442383\n",
      "152 24.384517669677734\n",
      "153 23.0396728515625\n",
      "154 21.7697696685791\n",
      "155 20.572004318237305\n",
      "156 19.44100570678711\n",
      "157 18.373008728027344\n",
      "158 17.365283966064453\n",
      "159 16.412967681884766\n",
      "160 15.513724327087402\n",
      "161 14.665504455566406\n",
      "162 13.864340782165527\n",
      "163 13.107009887695312\n",
      "164 12.391586303710938\n",
      "165 11.716715812683105\n",
      "166 11.07868766784668\n",
      "167 10.476027488708496\n",
      "168 9.907144546508789\n",
      "169 9.36937141418457\n",
      "170 8.861063003540039\n",
      "171 8.380958557128906\n",
      "172 7.927491664886475\n",
      "173 7.49874210357666\n",
      "174 7.0934014320373535\n",
      "175 6.710973262786865\n",
      "176 6.348963737487793\n",
      "177 6.006668567657471\n",
      "178 5.683293342590332\n",
      "179 5.377862930297852\n",
      "180 5.08894157409668\n",
      "181 4.81574821472168\n",
      "182 4.557445049285889\n",
      "183 4.313446044921875\n",
      "184 4.082497596740723\n",
      "185 3.8641531467437744\n",
      "186 3.657533884048462\n",
      "187 3.4623496532440186\n",
      "188 3.2777528762817383\n",
      "189 3.1032192707061768\n",
      "190 2.9379124641418457\n",
      "191 2.7816433906555176\n",
      "192 2.633725166320801\n",
      "193 2.493936538696289\n",
      "194 2.361548900604248\n",
      "195 2.236473560333252\n",
      "196 2.118070125579834\n",
      "197 2.006068229675293\n",
      "198 1.8999532461166382\n",
      "199 1.7995760440826416\n",
      "200 1.7046737670898438\n",
      "201 1.6146819591522217\n",
      "202 1.5297503471374512\n",
      "203 1.4492442607879639\n",
      "204 1.3729554414749146\n",
      "205 1.300903558731079\n",
      "206 1.2325615882873535\n",
      "207 1.1679468154907227\n",
      "208 1.1067309379577637\n",
      "209 1.0487191677093506\n",
      "210 0.9938947558403015\n",
      "211 0.9420543909072876\n",
      "212 0.8928252458572388\n",
      "213 0.8461862802505493\n",
      "214 0.8021243810653687\n",
      "215 0.7602791786193848\n",
      "216 0.7207052707672119\n",
      "217 0.6832742094993591\n",
      "218 0.6477850079536438\n",
      "219 0.6142083406448364\n",
      "220 0.5823142528533936\n",
      "221 0.5521367788314819\n",
      "222 0.5235140323638916\n",
      "223 0.49642112851142883\n",
      "224 0.47079208493232727\n",
      "225 0.44644695520401\n",
      "226 0.42344218492507935\n",
      "227 0.40159374475479126\n",
      "228 0.38088345527648926\n",
      "229 0.36129286885261536\n",
      "230 0.34272080659866333\n",
      "231 0.32506847381591797\n",
      "232 0.3083471953868866\n",
      "233 0.29261526465415955\n",
      "234 0.2775752544403076\n",
      "235 0.26336240768432617\n",
      "236 0.2499004602432251\n",
      "237 0.23708035051822662\n",
      "238 0.22495734691619873\n",
      "239 0.2135065495967865\n",
      "240 0.20257829129695892\n",
      "241 0.19222429394721985\n",
      "242 0.18247652053833008\n",
      "243 0.1732250154018402\n",
      "244 0.16439278423786163\n",
      "245 0.15602004528045654\n",
      "246 0.1480867564678192\n",
      "247 0.14058606326580048\n",
      "248 0.13346350193023682\n",
      "249 0.12668977677822113\n",
      "250 0.12027668207883835\n",
      "251 0.11420569568872452\n",
      "252 0.10843525826931\n",
      "253 0.1029563844203949\n",
      "254 0.09774114191532135\n",
      "255 0.09279985725879669\n",
      "256 0.08814819902181625\n",
      "257 0.08371026813983917\n",
      "258 0.07950812578201294\n",
      "259 0.0755062997341156\n",
      "260 0.07170695066452026\n",
      "261 0.06810247898101807\n",
      "262 0.0646965354681015\n",
      "263 0.061470262706279755\n",
      "264 0.05839017778635025\n",
      "265 0.05546931177377701\n",
      "266 0.05270378664135933\n",
      "267 0.05007697641849518\n",
      "268 0.04758935049176216\n",
      "269 0.045199327170848846\n",
      "270 0.042947299778461456\n",
      "271 0.04080624505877495\n",
      "272 0.03878336772322655\n",
      "273 0.03685875982046127\n",
      "274 0.03503509238362312\n",
      "275 0.033294130116701126\n",
      "276 0.031637005507946014\n",
      "277 0.030079631134867668\n",
      "278 0.028595663607120514\n",
      "279 0.02719050645828247\n",
      "280 0.02584705874323845\n",
      "281 0.02457997389137745\n",
      "282 0.023354588076472282\n",
      "283 0.022207267582416534\n",
      "284 0.02112714760005474\n",
      "285 0.020089218392968178\n",
      "286 0.019107520580291748\n",
      "287 0.018180757761001587\n",
      "288 0.01729006879031658\n",
      "289 0.016448376700282097\n",
      "290 0.01565723493695259\n",
      "291 0.01488740835338831\n",
      "292 0.014164427295327187\n",
      "293 0.013485610485076904\n",
      "294 0.012834785506129265\n",
      "295 0.012215152382850647\n",
      "296 0.011627770960330963\n",
      "297 0.011067131534218788\n",
      "298 0.010532849468290806\n",
      "299 0.010034339502453804\n",
      "300 0.009558172896504402\n",
      "301 0.00910043716430664\n",
      "302 0.008670290000736713\n",
      "303 0.008256614208221436\n",
      "304 0.00787055678665638\n",
      "305 0.007505159359425306\n",
      "306 0.007149919867515564\n",
      "307 0.006810653489083052\n",
      "308 0.006491892039775848\n",
      "309 0.0061945002526044846\n",
      "310 0.005906993523240089\n",
      "311 0.005634197499603033\n",
      "312 0.005374367814511061\n",
      "313 0.005127826239913702\n",
      "314 0.0048971157521009445\n",
      "315 0.00467410683631897\n",
      "316 0.0044608344323933125\n",
      "317 0.004260570742189884\n",
      "318 0.004068837966769934\n",
      "319 0.003891835454851389\n",
      "320 0.003717774758115411\n",
      "321 0.0035538861993700266\n",
      "322 0.00339978514239192\n",
      "323 0.0032500189263373613\n",
      "324 0.003110822755843401\n",
      "325 0.002977780532091856\n",
      "326 0.0028469436801970005\n",
      "327 0.0027281700167804956\n",
      "328 0.0026114140637218952\n",
      "329 0.0025015929713845253\n",
      "330 0.0023971826303750277\n",
      "331 0.0022962004877626896\n",
      "332 0.0022031667176634073\n",
      "333 0.0021134635899215937\n",
      "334 0.0020283227786421776\n",
      "335 0.0019457109738141298\n",
      "336 0.0018670602003112435\n",
      "337 0.0017925361171364784\n",
      "338 0.0017225380288437009\n",
      "339 0.001655571861192584\n",
      "340 0.0015915187541395426\n",
      "341 0.00153034133836627\n",
      "342 0.0014720046892762184\n",
      "343 0.0014157250989228487\n",
      "344 0.0013636136427521706\n",
      "345 0.0013116467744112015\n",
      "346 0.0012619976187124848\n",
      "347 0.0012156795710325241\n",
      "348 0.0011707355733960867\n",
      "349 0.0011274834396317601\n",
      "350 0.0010863825445994735\n",
      "351 0.0010474736336618662\n",
      "352 0.0010098664788529277\n",
      "353 0.0009740974055603147\n",
      "354 0.0009389667538926005\n",
      "355 0.0009067028295248747\n",
      "356 0.0008751849527470767\n",
      "357 0.0008458911906927824\n",
      "358 0.0008159042336046696\n",
      "359 0.0007888792315497994\n",
      "360 0.0007622971897944808\n",
      "361 0.0007368485676124692\n",
      "362 0.0007128220167942345\n",
      "363 0.0006896784761920571\n",
      "364 0.0006680403603240848\n",
      "365 0.0006459165597334504\n",
      "366 0.0006246311240829527\n",
      "367 0.000603847554884851\n",
      "368 0.0005852551548741758\n",
      "369 0.0005659190937876701\n",
      "370 0.0005496133817359805\n",
      "371 0.0005324122030287981\n",
      "372 0.000514691520947963\n",
      "373 0.000499853806104511\n",
      "374 0.00048503815196454525\n",
      "375 0.0004710946814157069\n",
      "376 0.0004566352581605315\n",
      "377 0.000443062192061916\n",
      "378 0.0004303406458348036\n",
      "379 0.0004174025962129235\n",
      "380 0.0004053281154483557\n",
      "381 0.00039397881482727826\n",
      "382 0.0003826008760370314\n",
      "383 0.00037198790232650936\n",
      "384 0.000361026672180742\n",
      "385 0.00035174842923879623\n",
      "386 0.0003416544641368091\n",
      "387 0.00033224746584892273\n",
      "388 0.0003233915485907346\n",
      "389 0.00031482279882766306\n",
      "390 0.00030588964000344276\n",
      "391 0.000298050552373752\n",
      "392 0.0002894505742006004\n",
      "393 0.0002821131784003228\n",
      "394 0.0002744485391303897\n",
      "395 0.00026797852478921413\n",
      "396 0.0002613572287373245\n",
      "397 0.0002549309574533254\n",
      "398 0.0002480719704180956\n",
      "399 0.00024211907293647528\n",
      "400 0.00023611939104739577\n",
      "401 0.00023021369997877628\n",
      "402 0.00022443599300459027\n",
      "403 0.00021925210603512824\n",
      "404 0.00021356987417675555\n",
      "405 0.00020894123008474708\n",
      "406 0.00020349878468550742\n",
      "407 0.00019904182408936322\n",
      "408 0.00019429771055001765\n",
      "409 0.00018941055168397725\n",
      "410 0.0001856864255387336\n",
      "411 0.00018135666323360056\n",
      "412 0.0001773745461832732\n",
      "413 0.00017380122153554112\n",
      "414 0.0001699043350527063\n",
      "415 0.0001662774884607643\n",
      "416 0.00016256771050393581\n",
      "417 0.00015876120596658438\n",
      "418 0.0001556309434818104\n",
      "419 0.00015233838348649442\n",
      "420 0.00014920736430212855\n",
      "421 0.000146648904774338\n",
      "422 0.00014336159802041948\n",
      "423 0.00014063750859349966\n",
      "424 0.00013750733342021704\n",
      "425 0.00013494718587026\n",
      "426 0.0001319236762356013\n",
      "427 0.00012936702114529908\n",
      "428 0.00012678344501182437\n",
      "429 0.0001243098231498152\n",
      "430 0.00012147268716944382\n",
      "431 0.00011950037151109427\n",
      "432 0.00011719409667421132\n",
      "433 0.00011486433504614979\n",
      "434 0.00011224352056160569\n",
      "435 0.00011023893603123724\n",
      "436 0.00010863073111977428\n",
      "437 0.00010621639376040548\n",
      "438 0.00010440556798130274\n",
      "439 0.00010243599535897374\n",
      "440 0.0001006032689474523\n",
      "441 9.8911885288544e-05\n",
      "442 9.7260570328217e-05\n",
      "443 9.553821291774511e-05\n",
      "444 9.375323861604556e-05\n",
      "445 9.208792471326888e-05\n",
      "446 9.032739035319537e-05\n",
      "447 8.907773008104414e-05\n",
      "448 8.711674308869988e-05\n",
      "449 8.562173024984077e-05\n",
      "450 8.430903835687786e-05\n",
      "451 8.294823783216998e-05\n",
      "452 8.137655822793022e-05\n",
      "453 8.0280595284421e-05\n",
      "454 7.886927051004022e-05\n",
      "455 7.770603406243026e-05\n",
      "456 7.632991764694452e-05\n",
      "457 7.532365270890296e-05\n",
      "458 7.386894867522642e-05\n",
      "459 7.278659904841334e-05\n",
      "460 7.156579522415996e-05\n",
      "461 7.048399129416794e-05\n",
      "462 6.93220499670133e-05\n",
      "463 6.835634121671319e-05\n",
      "464 6.731490429956466e-05\n",
      "465 6.635140744037926e-05\n",
      "466 6.542519258800894e-05\n",
      "467 6.42563245492056e-05\n",
      "468 6.333034252747893e-05\n",
      "469 6.239017966436222e-05\n",
      "470 6.155343726277351e-05\n",
      "471 6.06368193984963e-05\n",
      "472 5.9628524468280375e-05\n",
      "473 5.885665814275853e-05\n",
      "474 5.816618795506656e-05\n",
      "475 5.7441862736595795e-05\n",
      "476 5.660371243720874e-05\n",
      "477 5.5993143178056926e-05\n",
      "478 5.4963278671493754e-05\n",
      "479 5.426329153124243e-05\n",
      "480 5.361954390536994e-05\n",
      "481 5.272305861581117e-05\n",
      "482 5.192535536480136e-05\n",
      "483 5.1476141379680485e-05\n",
      "484 5.103541479911655e-05\n",
      "485 5.045862781116739e-05\n",
      "486 4.96806314913556e-05\n",
      "487 4.9162998038809747e-05\n",
      "488 4.83996445836965e-05\n",
      "489 4.78322763228789e-05\n",
      "490 4.7162920964183286e-05\n",
      "491 4.64911681774538e-05\n",
      "492 4.595121572492644e-05\n",
      "493 4.54216860816814e-05\n",
      "494 4.4978187361266464e-05\n",
      "495 4.446699676918797e-05\n",
      "496 4.389759851619601e-05\n",
      "497 4.355708369985223e-05\n",
      "498 4.2919964471366256e-05\n",
      "499 4.239912232151255e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward prop\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0) # clamp：夹紧，所以意思你懂的\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # backward prop\n",
    "    grad_y_pred = 2 * (y_pred-y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t()) # .t() 等于 numpy 中的.T,及转置\n",
    "    grad_h = grad_h_relu.clone()# .clone 等于numpy 中的.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch：Tensors和autograd\n",
    "在上面的例子中，我们不得不手动实现神经网络的前向和后向传递。手动实现反向传递对于小型双层网络来说并不是什么大问题，但对于大型复杂网络来说很快就会变得非常繁琐。\n",
    "\n",
    "值得庆幸的是，我们可以使用自动微分 来自动计算神经网络中的后向传递。PyTorch中的 autograd包提供了这个功能。使用autograd时，网络的正向传递将定义 计算图形 ; 图中的节点将是张量，边将是从输入张量产生输出张量的函数。通过此图反向传播，您可以轻松计算渐变。\n",
    "\n",
    "这听起来很复杂，在实践中使用起来非常简单。每个Tensor表示计算图中的节点。如果x是Tensor x.requires_grad=True那么x.grad是另一个Tensor持有x相对于某个标量值的梯度。\n",
    "\n",
    "在这里，我们使用PyTorch Tensors和autograd来实现我们的双层网络; 现在我们不再需要手动实现通过网络的反向传递："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1734e-06, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "2.735474109649658 s\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "t1 = time.time()\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 设定 requires_grad=False 表明我们在反向传播中不需要计算相应的张量的梯度\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 设定 requires_grad=True 表明我们在反向传播中会计算相应的张量的梯度\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # forword prop\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # loss function\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "#     print(t, loss.item())\n",
    "    \n",
    "    # 利用autograd去计算loss函数的反向传播中所有梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新权重\n",
    "    # 这里选择利用梯度下降手动更新权重，当然也可以使用torch.optim.SGD进行梯度下降\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad #因为前面的.backward,所以这里w1.grad直接获取到w1的梯度\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 手动将梯度归零,否则梯度会累加，这是pytorch动态图的特点之一\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        \n",
    "t2 = time.time()\n",
    "print(loss)\n",
    "print(t2-t1,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch：定义新的autograd函数\n",
    "每个原始 autograd 运算符实际上是两个在Tensors上运行的函数.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
